---
title: "Update DE Retailers - Markdown"
author: "Kathleen Scopis"
date: "`r Sys.Date()`"
output: html_document
---

## Introduction
The Geoscanning project 

The following code loads, cleans, and geocodes retailers licensed in Delaware to be used in the Geoscanning project.
Authored by Michael Fichman and Kathleen Scopis.

Before beginning the analysis, look over the dataset you are looking to clean and become familiar with the column names and data types.  You will need to insert the unique Google Geocoding key in the chunk below in order to properly geocode.  NOTE: DO NOT INCLUDE KEY IN ANY PUBLICLY AVAILABLE CODE.

To begin, load in essential libraries and specify Google Geocoding Key

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(sf)
library(tigris)
library(tidycensus)
library(lubridate)
library(ggmap)
library(jsonlite)
library(RSocrata)
library(readxl)
library(leaflet)
library(leaflet.providers)
library(readxl)

# Load API key

register_google(key = "YOUR KEY GOES HERE")

```


Once libraries are loaded and API has been defined, it's time to perform the analysis.
Begin by loading in the canonical names (column names - these may changed based on dataset)

```{r}

# Load canonical names

canonical_names <- c("county", "trade_name", "account", "license_type",
                     "expiration_date", "lat", "lon", "address_full",
                     "publish_date", "state", "expired_y_n")

# Load states for trouble shooting

states_shp <- states()  %>%
  st_as_sf() %>%
  st_transform(crs = 4326)

pa_shp <- states_shp %>%
  filter(STUSPS == "PA")

nj_shp <- states_shp %>%
  filter(STUSPS == "NJ")

de_shp <- states_shp %>%
  filter(STUSPS == "DE")


## NOTE: categories have changed for how they describe tobacco retailers!

```


Next, pull in updated list of retailers from Delaware's online database, found on the State's official website.  Rename columns to fit canonical names as listed above.
Pull data from updated static list (found in Box folder, typically in Excel format).  
Join online dataset with static list and remove duplicate values - this will create the final dataset to be used for later analysis.

``` {r}

# Load online data using socrata package
 
retailers_Socrata_de <- read.socrata("https://data.delaware.gov/resource/5zy2-grhr.json") %>%
  filter(str_detect(category, "CIGARETTE/TOBACCO PRODUCTS SELLER") | 
           str_detect(category, "TOBACCO RETAILER")) %>%
  mutate(publish_date = ymd(current_license_valid_from),
         expiration_date = ymd(current_license_valid_to),
         expired_y_n = "ACTIVE",
         trade_name = business_name,
         county = NA,
         state = "DE") %>%
  rename(lat = geocoded_location.latitude,
         lon = geocoded_location.longitude,
         account = license_number,
         license_type = category) %>%
  mutate(zip = ifelse(str_length(zip) == 9, str_sub(zip, end = -5), zip)) %>%
  filter(zip >= 19701 & zip <= 19980) %>%
  unite(., "address_full", sep = " ",
        c("address_1", "city", "state", "zip")) %>%
  mutate(state = "DE") %>%
  dplyr::select(-business_name, 
                -current_license_valid_from, 
                -current_license_valid_to, 
                -country, 
                -geocoded_location.human_address
                ) %>% #removed -address_1 (KS)
  mutate_if(is.factor, as.character) %>%
  mutate(address_full = str_replace(address_full, "&", "AND"))

# Load stored data

stored_de <- read.csv("Data/Retailers/DE/05_08_23_Delaware_Business_Licenses.csv") %>%
  mutate_if(is.factor, as.character) %>%
  rename(trade_name = Trade.name,
         account = License.number)

stored_de$expiration = str_replace_all(stored_de$Current.license.valid.to, "\\/", "-")
stored_de$published = str_replace_all(stored_de$Current.license.valid.from, "\\/", "-")
stored_de = stored_de %>% 
  mutate(expiration_date = as.Date(expiration, 
                                   tryFormats = c("%m-%d-%Y", "%Y-%m-%d")),
         publish_date = as.Date(published, 
                                tryFormats = c("%m-%d-%Y", "%Y-%m-%d")),
        account = as.character(account)
        )

# Join new and stored data

joined_de <- full_join(stored_de, 
                       retailers_Socrata_de, by = c( "account", "trade_name")) %>%
  mutate(expired_y_n = ifelse(is.na(expiration_date.y) == TRUE, "EXPIRED", "ACTIVE"),
         expiration_date = if_else(expired_y_n == "EXPIRED", expiration_date.x, ymd(expiration_date.y)),
         publish_date = if_else(is.na(publish_date.y) == TRUE, publish_date.x, publish_date.y),
         state = "DE") %>%
  dplyr::select(
                -publish_date.x, 
                -publish_date.y, 
                -expiration_date.x, 
                -expiration_date.y
                )

```


Once the joined dataset is created, it's time to begin the geocoding process.  "Geocoding" refers to the assignment of lat/long coordinate points to any given address.  The code below creates the "address_full" column, which will be used to search online for coordinate points.

The final dataset is then broken out into 13 groups.  Batch processing is crucial when geocoding particularly large datasets, as it decreases computational load.  (When running previous, un-batched tests, R crashed and was unsuccessful)


``` {r}



# Create "Address_full" column to joined table - used in geocoding

joined_de$address_full <- paste(joined_de$Address.1, 
                                joined_de$City, 
                                joined_de$state,
                                joined_de$Zip)

# Partition data for faster loading

joined_de_1 <- joined_de[1:5000,]
joined_de_2 <- joined_de[5001:10000,]
joined_de_3 <- joined_de[10001:15000,]
joined_de_4 <- joined_de[15001:20000,]
joined_de_5 <- joined_de[20001:25000,]
joined_de_6 <- joined_de[25001:30000,]
joined_de_7 <- joined_de[30001:35000,]
joined_de_8 <- joined_de[35001:40000,]
joined_de_9 <- joined_de[40001:45000,]
joined_de_10 <- joined_de[45001:50000,]
joined_de_11 <- joined_de[50001:55000,]
joined_de_12 <- joined_de[55001:60000,]
joined_de_13 <- joined_de[60001:62956,]



# Geocode missing info

to_geocode_de_1 <- joined_de_1 %>%
  filter(is.na(lat) == TRUE)

geocoded_de_1 <- geocode(to_geocode_de_1$address_full, source =  "google") %>%
  cbind(., to_geocode_de_1 %>%
          select(-lat, -lon)) %>%
  rbind(joined_de_1 %>%
          filter(is.na(lat) == FALSE), .)



to_geocode_de_2 <- joined_de_2 %>%
  filter(is.na(lat) == TRUE)

geocoded_de_2 <- geocode(to_geocode_de_2$address_full, source =  "google") %>%
  cbind(., to_geocode_de_2 %>%
          select(-lat, -lon)) %>%
  rbind(joined_de_2 %>%
          filter(is.na(lat) == FALSE), .)



to_geocode_de_3 <- joined_de_3 %>%
  filter(is.na(lat) == TRUE)

geocoded_de_3 <- geocode(to_geocode_de_3$address_full, source =  "google") %>%
  cbind(., to_geocode_de_3 %>%
          select(-lat, -lon)) %>%
  rbind(joined_de_3 %>%
          filter(is.na(lat) == FALSE), .)



to_geocode_de_4 <- joined_de_4 %>%
  filter(is.na(lat) == TRUE)

geocoded_de_4 <- geocode(to_geocode_de_4$address_full, source =  "google") %>%
  cbind(., to_geocode_de_4 %>%
          select(-lat, -lon)) %>%
  rbind(joined_de_4 %>%
          filter(is.na(lat) == FALSE), .)



to_geocode_de_5 <- joined_de_5 %>%
  filter(is.na(lat) == TRUE)

geocoded_de_5 <- geocode(to_geocode_de_5$address_full, source =  "google") %>%
  cbind(., to_geocode_de_5 %>%
          select(-lat, -lon)) %>%
  rbind(joined_de_5 %>%
          filter(is.na(lat) == FALSE), .)



to_geocode_de_6 <- joined_de_6 %>%
  filter(is.na(lat) == TRUE)

geocoded_de_6 <- geocode(to_geocode_de_6$address_full, source =  "google") %>%
  cbind(., to_geocode_de_6 %>%
          select(-lat, -lon)) %>%
  rbind(joined_de_6 %>%
          filter(is.na(lat) == FALSE), .)



to_geocode_de_7 <- joined_de_7 %>%
  filter(is.na(lat) == TRUE)

geocoded_de_7 <- geocode(to_geocode_de_7$address_full, source =  "google") %>%
  cbind(., to_geocode_de_7 %>%
          select(-lat, -lon)) %>%
  rbind(joined_de_7 %>%
          filter(is.na(lat) == FALSE), .)



to_geocode_de_8 <- joined_de_8 %>%
  filter(is.na(lat) == TRUE)

geocoded_de_8 <- geocode(to_geocode_de_8$address_full, source =  "google") %>%
  cbind(., to_geocode_de_8 %>%
          select(-lat, -lon)) %>%
  rbind(joined_de_8 %>%
          filter(is.na(lat) == FALSE), .)



to_geocode_de_9 <- joined_de_9 %>%
  filter(is.na(lat) == TRUE)

geocoded_de_9 <- geocode(to_geocode_de_9$address_full, source =  "google") %>%
  cbind(., to_geocode_de_9 %>%
          select(-lat, -lon)) %>%
  rbind(joined_de_9 %>%
          filter(is.na(lat) == FALSE), .)



to_geocode_de_10 <- joined_de_10 %>%
  filter(is.na(lat) == TRUE)

geocoded_de_10 <- geocode(to_geocode_de_10$address_full, source =  "google") %>%
  cbind(., to_geocode_de_10 %>%
          select(-lat, -lon)) %>%
  rbind(joined_de_10 %>%
          filter(is.na(lat) == FALSE), .)



to_geocode_de_11 <- joined_de_11 %>%
  filter(is.na(lat) == TRUE)

geocoded_de_11 <- geocode(to_geocode_de_11$address_full, source =  "google") %>%
  cbind(., to_geocode_de_11 %>%
          select(-lat, -lon)) %>%
  rbind(joined_de_11 %>%
          filter(is.na(lat) == FALSE), .)



to_geocode_de_12 <- joined_de_12 %>%
  filter(is.na(lat) == TRUE)

geocoded_de_12 <- geocode(to_geocode_de_12$address_full, source =  "google") %>%
  cbind(., to_geocode_de_12 %>%
          select(-lat, -lon)) %>%
  rbind(joined_de_12 %>%
          filter(is.na(lat) == FALSE), .)



to_geocode_de_13 <- joined_de_13 %>%
  filter(is.na(lat) == TRUE)

geocoded_de_13 <- geocode(to_geocode_de_13$address_full, source =  "google") %>%
  cbind(., to_geocode_de_13 %>%
          select(-lat, -lon)) %>%
  rbind(joined_de_13 %>%
          filter(is.na(lat) == FALSE), .)

summary(is.na(geocoded_de$lat))

# Merge all batches into final ouput dataset

merge_de_1 <- geocoded_de_1
merge_de_2 <- geocoded_de_2
merge_de_3 <- geocoded_de_3
merge_de_4 <- geocoded_de_4
merge_de_5 <- geocoded_de_5
merge_de_6 <- geocoded_de_6
merge_de_7 <- geocoded_de_7
merge_de_8 <- geocoded_de_8
merge_de_9 <- geocoded_de_9
merge_de_10 <- geocoded_de_10
merge_de_11 <- geocoded_de_11
merge_de_12 <- geocoded_de_12
merge_de_13 <- geocoded_de_13


final_de <- rbind(merge_de_1[1:5000, ], 
                  merge_de_2[1:5000, ], 
                  merge_de_3[1:5000, ],
                  merge_de_4[1:5000, ],
                  merge_de_5[1:5000, ],
                  merge_de_6[1:5000, ], 
                  merge_de_7[1:5000, ], 
                  merge_de_8[1:5000, ],
                  merge_de_9[1:5000, ],
                  merge_de_10[1:5000, ], 
                  merge_de_11[1:5000, ], 
                  merge_de_12[1:5000, ],
                  merge_de_13[1:2956, ]
                  )

write.csv(final_de, file = "de_data_cleaned.csv", row.names = TRUE)

```

Quality-control check the dataset for any errors and incorrect coordinate results.

``` {r}


# There are no NA observations, so we do this
geocoded_de <- joined_de

# Check for bad geocodes

errors_de <- st_join(geocoded_de %>% 
                       filter(is.na(lat) == FALSE) %>% 
                       st_as_sf(., coords = c("lon", "lat"), crs = 4326), 
                     de_shp, 
                     join = st_within, 
                     left = TRUE) %>%
  filter(is.na(STUSPS) == TRUE) %>%
  mutate(lon=map_dbl(geometry, ~st_centroid(.x)[[1]]),
         lat=map_dbl(geometry, ~st_centroid(.x)[[2]]))%>%
  dplyr::select(canonical_names) %>%
  as.data.frame() %>%
  dplyr::select(-geometry) %>%
  rbind(., geocoded_de %>%
          filter(is.na(lat) == TRUE))

# Manually fix bad geocodes

geocoded_de_fixed <- geocoded_de %>%
  mutate(lat = ifelse(account == "2005208433", 39.64222847614057, lat),
         lon = ifelse(account == "2005208433", -75.64473202156881, lon))

```

Lastly, pull the cleaned dataset back together and export.  (You may need to update file path to direct to the correct folder)

``` {r}

# Put all the data back together

allStates_updated <- read.csv("~/GitHub/geoscanning/Data/Retailers/all_Retailers_5_26_21.csv") %>%
  dplyr::select(canonical_names) %>%
  mutate_if(is.factor, as.character) %>%
  mutate(account = as.character(account)) %>%
  mutate(expiration_date = ymd(expiration_date),
         publish_date = ymd(publish_date)) %>%
  filter(state != "DE") %>%
  rbind(., geocoded_de_fixed)

write.csv(allStates_updated, "~/GitHub/geoscanning/Data/Retailers/all_Retailers_5_26_21.csv")

```

