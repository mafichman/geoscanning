---
title: "Update PA Retailers - Markdown"
author: "Kathleen Scopis"
date: "`r Sys.Date()`"
output: html_document
---

## Introduction
The Geoscanning project 

The following code loads, cleans, and geocodes retailers licensed in Pennsylvania to be used in the Geoscanning project.
Authored by Michael Fichman and Kathleen Scopis.

Before beginning the analysis, look over the dataset you are looking to clean and become familiar with the column names and data types.  You will need to insert the unique Google Geocoding key in the chunk below in order to properly geocode.  NOTE: DO NOT INCLUDE KEY IN ANY PUBLICLY AVAILABLE CODE.

To begin, load in essential libraries and specify Google Geocoding Key.

``` {r}

# load packages

library(tidyverse)
library(sf)
library(tigris)
library(tidycensus)
library(lubridate)
library(ggmap)
library(jsonlite)
library(RSocrata)
library(readxl)
library(leaflet)
library(leaflet.providers)
library(readxl)

# Load API key

register_google(key = "YOUR KEY GOES HERE")

```

Once libraries are loaded and API has been defined, it's time to perform the analysis.
Begin by loading in the canonical names (column names - these may changed based on dataset)

``` {r}
# Load canonical names

canonical_names <- c("county", "trade_name", "account", "license_type",
                     "expiration_date", "lat", "lon", "address_full",
                     "publish_date", "state", "expired_y_n")

# Load states for trouble shooting

states_shp <- states()  %>%
  st_as_sf() %>%
  st_transform(crs = 4326)

pa_shp <- states_shp %>%
  filter(STUSPS == "PA")

nj_shp <- states_shp %>%
  filter(STUSPS == "NJ")

de_shp <- states_shp %>%
  filter(STUSPS == "DE")

```

Next, pull in updated list of retailers from Delaware's online database, found on the State's official website.  Rename columns to fit canonical names as listed above.
Pull data from updated static list (found in Box folder, typically in Excel format).  
Join online dataset with static list and remove duplicate values - this will create the final dataset to be used for later analysis.

``` {r}

# Load online data using socrata package

retailers_Socrata_pa <- read.socrata("https://data.pa.gov/resource/ut72-sft8.json") %>%
  filter(county != "UNKNOWN/OUT OF STATE",
         license_type != "Vending") %>%
  mutate(publish_date = today(tzone = "America/New_York"),
         state = "PA",
         expiration_date = ymd(expiration_date)) %>%
  rename(lat = location_1.latitude,
         lon = location_1.longitude,
         address_full = location_1.human_address) %>%
  dplyr::select(-legal_name, -postal_code, -country) %>%
  mutate_if(is.factor, as.character) %>%
  mutate(address_full = str_replace(address_full, "&", "AND"))

# Load stored retailer database, filter for only PA observations

stored_pa <- read.csv("Data/Retailers/PA/Tobacco_Products_Tax_Licenses_Current_Monthly_County_Revenue.csv") %>%
  ##dplyr::select(canonical_names) %>%
  mutate_if(is.factor, as.character) %>%
  rename(trade_name = Trade.Name,
         account = Account.Code,
         license_type = License.Type,
         county = County)

stored_pa$expiration = str_replace_all(stored_pa$Expiration.Date, "\\/", "-")
stored_pa = stored_pa %>% 
  mutate(expiration_date = as.Date(expiration, tryFormats = c("%m-%d-%Y", "%Y-%m-%d")))


# Join the new and the stored data

joined_pa <- full_join(stored_pa, 
                       retailers_Socrata_pa, by = c( "county",
                                                     "trade_name",
                                                     "account",
                                                     "license_type", 
                                                     "expiration_date"
                                                     ) ) %>%


# Example code to check which rows have duplicate matches
subset(stored_pa, trade_name == "AMERICAN LEGION")
subset(retailers_Socrata_pa, trade_name == "PINKYS VENDING INC")
subset(retailers_Socrata_pa, trade_name == "AMERICAN LEGION")
subset(retailers_Socrata_pa[c(13331),])
subset(stored_pa[c(100),])

```

Once the joined dataset is created, it's time to begin the geocoding process.  "Geocoding" refers to the assignment of lat/long coordinate points to any given address.  The code below creates the "address_full" column, which will be used to search online for coordinate points.

``` {r}

# See how many NAs we have in our geolocations

summary(is.na(joined_pa$lat))

# Do a first pass at geocoding

to_geocode_pa <- joined_pa %>%
  filter(is.na(lat) == TRUE)

geocoded_pa <- geocode(to_geocode_pa$Address...Lat.Long, source =  "google") %>%
  cbind(., to_geocode_pa %>%
          select(-lat, -lon)) %>%
  rbind(joined_pa %>%
          filter(is.na(lat) == FALSE), .)


summary(is.na(geocoded_pa$lat))

# Perform an error check for bad geocodes

errors_pa <- st_join(geocoded_pa %>% 
                       filter(is.na(lat) == FALSE) %>% 
                       st_as_sf(., coords = c("lon", "lat"), crs = 4326), 
                     pa_shp, 
                     join = st_within, 
                     left = TRUE) %>%
  filter(is.na(STUSPS) == TRUE) %>%
  mutate(lon=map_dbl(geometry, ~st_centroid(.x)[[1]]),
         lat=map_dbl(geometry, ~st_centroid(.x)[[2]]))%>%
 # dplyr::select(canonical_names) %>%
  as.data.frame() %>%
  dplyr::select(-geometry) %>%
  rbind(., geocoded_pa %>%
          filter(is.na(lat) == TRUE))

#Send CSV to working directory folder

write.csv(geocoded_pa, "geocoded_pa.csv")

```

Quality-control check the dataset for any errors and incorrect coordinate results.

```{r}
# Manually geocode failures

geocoded_pa_fixed <- geocoded_pa %>%
  mutate(lat = ifelse(account %in% c("02**1873", "33**3114"), NA, lat),
         lon = ifelse(account %in% c("02**1873", "33**3114"), NA, lon)) %>%
  mutate(lat = ifelse(account == "70**5157", 42.074211070532854,  lat),
         lon = ifelse(account == "70**5157", -80.14327133127297, lon)) %>%
  mutate(lat = ifelse(account %in% c("25**1938", "25**8306"), 42.210053022150504,   lat),
         lon = ifelse(account %in% c("25**1938", "25**8306"), -79.84988587711423, lon)) %>%
  mutate(lat = ifelse(account == "09**0009", 40.12615975272745,  lat),
         lon = ifelse(account == "09**0009", -74.83672149886412,  lon)) %>%
  mutate(lat = ifelse(account == "40**3538", 41.33283809763932,  lat),
         lon = ifelse(account == "40**3538", -75.95459226289694,  lon)) %>%
  mutate(lat = ifelse(account == "09**2991", 40.10934146246511,   lat),
         lon = ifelse(account == "09**2991", -74.95117508837193,  lon))

```

Lastly, pull the cleaned dataset back together and export.  (You may need to update file path to direct to the correct folder)

``` {r}
# Append to the rest of the retailers and write it out

allStates_updated <- read.csv("~/GitHub/geoscanning/Data/Retailers/all_Retailers_5_20_21.csv") %>%
  dplyr::select(canonical_names) %>%
  mutate_if(is.factor, as.character) %>%
  mutate(account = as.character(account)) %>%
  mutate(expiration_date = ymd(expiration_date),
         publish_date = ymd(publish_date)) %>%
  filter(state != "PA") %>%
  rbind(., geocoded_pa_fixed)

write.csv(allStates_updated, "~/GitHub/geoscanning/Data/Retailers/all_Retailers_5_26_21.csv")
```
